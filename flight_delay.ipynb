{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Predicting airline delays with Spark and ML-Lib using pySpark </h1>\n",
    "<br>\n",
    "Adapted from http://nbviewer.ipython.org/github/ofermend/IPython-notebooks/blob/master/blog-part-2.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Pre-processing with PySpark</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'Year,Month,DayofMonth,DayOfWeek,DepTime,CRSDepTime,ArrTime,CRSArrTime,UniqueCarrier,FlightNum,TailNum,ActualElapsedTime,CRSElapsedTime,AirTime,ArrDelay,DepDelay,Origin,Dest,Distance,TaxiIn,TaxiOut,Cancelled,CancellationCode,Diverted,CarrierDelay,WeatherDelay,NASDelay,SecurityDelay,LateAircraftDelay']\n",
      "[u'2008,1,3,4,1738,1715,1838,1820,WN,82,N499WN,60,65,42,18,23,LAS,LAX,236,6,12,0,,0,0,0,0,12,6']\n"
     ]
    }
   ],
   "source": [
    "import os.path\n",
    "\n",
    "AIRPORT = 'LAX'\n",
    "\n",
    "base_dir = os.path.join('data')\n",
    "input_path_2007 = os.path.join('flights', '2007.csv')\n",
    "input_path_2008 = os.path.join('flights', '2008.csv')\n",
    "file_name_2007 = os.path.join(base_dir, input_path_2007)\n",
    "file_name_2008 = os.path.join(base_dir, input_path_2008)\n",
    "\n",
    "raw_data_2007 = sc.textFile(file_name_2007)\n",
    "raw_data_2008 = sc.textFile(file_name_2008)\n",
    "header = raw_data_2007.take(1) \n",
    "\n",
    "# filter on Airport\n",
    "filtered_data_2007 = (raw_data_2007\n",
    "                        .filter(lambda line: ',' + AIRPORT + ',' in line)\n",
    "                        # filter out cancelled flights\n",
    "                        .filter(lambda line: ',,' in line)\n",
    "                        .filter(lambda line: 'Year' not in line))\n",
    "filtered_data_2008 = (raw_data_2008\n",
    "                        .filter(lambda line: ',' + AIRPORT + ',' in line)\n",
    "                        # filter out cancelled flights\n",
    "                        .filter(lambda line: ',,' in line)\n",
    "                        .filter(lambda line: 'Year' not in line))\n",
    "\n",
    "# CRS = Computer Reservation System\n",
    "# scheduled time as opposed to the actual time\n",
    "print header\n",
    "print filtered_data_2008.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "holidays = ['01/01/2007', '01/15/2007', '02/19/2007', '05/28/2007', '06/07/2007', '07/04/2007',\n",
    "      '09/03/2007', '10/08/2007' ,'11/11/2007', '11/22/2007', '12/25/2007',\n",
    "      '01/01/2008', '01/21/2008', '02/18/2008', '05/22/2008', '05/26/2008', '07/04/2008',\n",
    "      '09/01/2008', '10/13/2008' ,'11/11/2008', '11/27/2008', '12/25/2008']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "def days_from_nearest_holiday(year, month, day):\n",
    "    diffs = []\n",
    "    sample_date = datetime.date(year, month, day)\n",
    "    for holiday in holidays:\n",
    "        dt = datetime.datetime.strptime(holiday, '%m/%d/%Y').date()\n",
    "        td = dt - sample_date\n",
    "        diffs.append(abs(td.days))\n",
    "\n",
    "    return min(diffs) * 1.0\n",
    "\n",
    "def split_data(line):\n",
    "    try:\n",
    "        vals = line.split(',')\n",
    "        features = []\n",
    "        features.append(float(vals[15]))\n",
    "        features.append(float(vals[1]))\n",
    "        features.append(float(vals[2]))\n",
    "        features.append(float(vals[3]))\n",
    "        features.append(float(vals[5]) / 100)\n",
    "        features.append(float(vals[18]))\n",
    "        features.append(days_from_nearest_holiday(int(vals[0]), int(vals[1]), int(vals[2])))\n",
    "\n",
    "        return (str(vals[0] + str(vals[1]) + str(vals[2])), features)\n",
    "    except:\n",
    "        return (str(vals[0] + str(vals[1]) + str(vals[2])), [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('200711', [1.0, 1.0, 1.0, 1.0, 14.45, 1593.0, 0.0])]\n"
     ]
    }
   ],
   "source": [
    "features_2007 = filtered_data_2007.map(lambda line: split_data(line))\n",
    "features_2008 = filtered_data_2008.map(lambda line: split_data(line))\n",
    "\n",
    "print features_2007.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Modeling with Spark and ML-Lib</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LabeledPoint(0.0, [-7.29720804563,0.0,-15.453017112,-2.81687181027,0.741804262984,931.822275593,-6.16121284899])]\n",
      "[LabeledPoint(1.0, [8.56169456377,0.0284402310769,-13.5271811583,0.204710413272,3.87748037328,-327.113612798,-3.18619463783])]\n"
     ]
    }
   ],
   "source": [
    "from pyspark.mllib.regression import LabeledPoint\n",
    "import numpy as np\n",
    "\n",
    "def parse_data(tup):\n",
    "    if tup[1][0] >= 15:\n",
    "        return LabeledPoint(1.0, tup[1])\n",
    "    else:\n",
    "        return LabeledPoint(0.0, tup[1])\n",
    "    \n",
    "def scale(data):\n",
    "    features_mean = data.map(lambda lp: lp.features).mean()\n",
    "    return data.map(lambda lp: LabeledPoint(lp.label, (lp.features - features_mean) / 1.0))\n",
    "\n",
    "train_data = features_2007.map(parse_data)\n",
    "train_data.cache()\n",
    "scaled_train_data = scale(train_data)\n",
    "scaled_train_data.cache()\n",
    "\n",
    "test_data = features_2008.map(parse_data)\n",
    "test_data.cache()\n",
    "scaled_test_data = scale(test_data)\n",
    "scaled_test_data.cache()\n",
    "\n",
    "print scaled_train_data.take(1)\n",
    "print scaled_test_data.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def eval_metrics(lbl_pred):\n",
    "    tp = float(lbl_pred.filter(lambda lp: lp[0]==1.0 and lp[1]==1.0).count())\n",
    "    tn = float(lbl_pred.filter(lambda lp: lp[0]==0.0 and lp[1]==0.0).count())\n",
    "    fp = float(lbl_pred.filter(lambda lp: lp[0]==1.0 and lp[1]==0.0).count())\n",
    "    fn = float(lbl_pred.filter(lambda lp: lp[0]==0.0 and lp[1]==1.0).count())\n",
    "    \n",
    "    precision = tp / (tp + fp)\n",
    "    recall = tp / (tp + fn)\n",
    "    F_measure = 2 * precision * recall / (precision + recall)\n",
    "    accuracy = (tp + tn) / (tp + tn + fp + fn)\n",
    "    return([tp, tn, fp, fn], [precision, recall, F_measure, accuracy])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision : 0.35\n",
      "Recall : 0.83\n",
      "F1 : 0.49\n",
      "Accuracy : 0.54\n"
     ]
    }
   ],
   "source": [
    "from pyspark.mllib.classification import LogisticRegressionWithSGD\n",
    "\n",
    "model_lr = LogisticRegressionWithSGD.train(scaled_train_data, iterations=100)\n",
    "labels_and_predictions = scaled_test_data.map(lambda lp: (model_lr.predict(lp.features), lp.label))\n",
    "metrics = eval_metrics(labels_and_predictions)\n",
    "\n",
    "print('Precision : %.2f' % round(metrics[1][0], 2))\n",
    "print('Recall : %.2f' % round(metrics[1][1], 2))\n",
    "print('F1 : %.2f' % round(metrics[1][2], 2))\n",
    "print('Accuracy : %.2f' % round(metrics[1][3], 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision : 0.38\n",
      "Recall : 0.84\n",
      "F1 : 0.53\n",
      "Accuracy : 0.60\n"
     ]
    }
   ],
   "source": [
    "from pyspark.mllib.classification import SVMWithSGD\n",
    "\n",
    "model_svm = SVMWithSGD.train(scaled_train_data, iterations=100, step=1.0, regParam=0.01)\n",
    "labels_and_predictions = scaled_test_data.map(lambda lp: (model_svm.predict(lp.features), lp.label))\n",
    "metrics = eval_metrics(labels_and_predictions)\n",
    "\n",
    "print('Precision : %.2f' % round(metrics[1][0], 2))\n",
    "print('Recall : %.2f' % round(metrics[1][1], 2))\n",
    "print('F1 : %.2f' % round(metrics[1][2], 2))\n",
    "print('Accuracy : %.2f' % round(metrics[1][3], 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision : 1.00\n",
      "Recall : 0.81\n",
      "F1 : 0.89\n",
      "Accuracy : 0.95\n"
     ]
    }
   ],
   "source": [
    "from pyspark.mllib.tree import DecisionTree\n",
    "\n",
    "# Experimental\n",
    "model_dt = DecisionTree.trainClassifier(scaled_train_data, 2, {}, 'gini', 10, 100)\n",
    "# this detour is needed because of the spark context error \n",
    "# if we map the test data set like in the previous models\n",
    "list_data = scaled_test_data.collect()\n",
    "list_predictions = []\n",
    "for lp in list_data:\n",
    "    list_predictions.append((model_dt.predict(lp.features), lp.label))\n",
    "\n",
    "labels_and_predictions = sc.parallelize(list_predictions)\n",
    "metrics = eval_metrics(labels_and_predictions)\n",
    "\n",
    "print('Precision : %.2f' % round(metrics[1][0], 2))\n",
    "print('Recall : %.2f' % round(metrics[1][1], 2))\n",
    "print('F1 : %.2f' % round(metrics[1][2], 2))\n",
    "print('Accuracy : %.2f' % round(metrics[1][3], 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision : 1.00\n",
      "Recall : 0.81\n",
      "F1 : 0.89\n",
      "Accuracy : 0.95\n"
     ]
    }
   ],
   "source": [
    "from pyspark.mllib.tree import RandomForest\n",
    "\n",
    "# Experimental\n",
    "model_rf = RandomForest.trainClassifier(scaled_train_data, 2, {}, 3, 'sqrt', 'gini', 10, 100)\n",
    "# this detour is needed because of the spark context error \n",
    "# if we map the test data set like in the previous models\n",
    "list_data = scaled_test_data.collect()\n",
    "list_predictions = []\n",
    "for lp in list_data:\n",
    "    list_predictions.append((model_rf.predict(lp.features), lp.label))\n",
    "\n",
    "labels_and_predictions = sc.parallelize(list_predictions)\n",
    "metrics = eval_metrics(labels_and_predictions)\n",
    "\n",
    "print('Precision : %.2f' % round(metrics[1][0], 2))\n",
    "print('Recall : %.2f' % round(metrics[1][1], 2))\n",
    "print('F1 : %.2f' % round(metrics[1][2], 2))\n",
    "print('Accuracy : %.2f' % round(metrics[1][3], 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<h3>Building a richer model with weather data</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "base_dir = os.path.join('data')\n",
    "input_path_LAX = os.path.join('flights', 'LAX.csv')\n",
    "file_name_LAX = os.path.join(base_dir, input_path_LAX)\n",
    "\n",
    "raw_data_LAX = sc.textFile(file_name_LAX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
