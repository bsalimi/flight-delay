{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Predicting airline delays with Spark and ML-Lib using pySpark </h1>\n",
    "<br>\n",
    "Adapted from http://nbviewer.ipython.org/github/ofermend/IPython-notebooks/blob/master/blog-part-2.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Pre-processing with PySpark</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'Year,Month,DayofMonth,DayOfWeek,DepTime,CRSDepTime,ArrTime,CRSArrTime,UniqueCarrier,FlightNum,TailNum,ActualElapsedTime,CRSElapsedTime,AirTime,ArrDelay,DepDelay,Origin,Dest,Distance,TaxiIn,TaxiOut,Cancelled,CancellationCode,Diverted,CarrierDelay,WeatherDelay,NASDelay,SecurityDelay,LateAircraftDelay']\n",
      "[u'2007,1,25,4,1052,1100,1359,1414,XE,1202,N12167,127,134,105,-15,-8,ORD,EWR,719,5,17,0,,0,0,0,0,0,0']\n",
      "[u'2008,1,8,2,1711,1600,2031,1945,XE,1226,N14953,140,165,106,46,71,ORD,EWR,719,8,26,0,,0,0,0,2,0,44']\n"
     ]
    }
   ],
   "source": [
    "import os.path\n",
    "\n",
    "AIRPORT = 'ORD'\n",
    "\n",
    "base_dir = os.path.join('data')\n",
    "# http://stat-computing.org/dataexpo/2009/the-data.html\n",
    "input_path_2007 = os.path.join('flights', '2007.csv')\n",
    "input_path_2008 = os.path.join('flights', '2008.csv')\n",
    "# http://www.ncdc.noaa.gov/cdo-web/datasets/\n",
    "input_path_weather = os.path.join('flights', AIRPORT + '.csv')\n",
    "\n",
    "file_name_2007 = os.path.join(base_dir, input_path_2007)\n",
    "file_name_2008 = os.path.join(base_dir, input_path_2008)\n",
    "file_name_weather = os.path.join(base_dir, input_path_weather)\n",
    "\n",
    "raw_data_2007 = sc.textFile(file_name_2007)\n",
    "raw_data_2008 = sc.textFile(file_name_2008)\n",
    "weather_data = sc.textFile(file_name_weather)\n",
    "\n",
    "header = raw_data_2007.take(1) \n",
    "\n",
    "filtered_data_2007 = (raw_data_2007\n",
    "                        # filter on Airport\n",
    "                        .filter(lambda line: ',' + AIRPORT + ',' in line)\n",
    "                        # filter out cancelled flights\n",
    "                        .filter(lambda line: ',,' in line)\n",
    "                        .filter(lambda line: 'Year' not in line))\n",
    "filtered_data_2008 = (raw_data_2008\n",
    "                        # filter on Airport\n",
    "                        .filter(lambda line: ',' + AIRPORT + ',' in line)\n",
    "                        # filter out cancelled flights\n",
    "                        .filter(lambda line: ',,' in line)\n",
    "                        .filter(lambda line: 'Year' not in line))\n",
    "\n",
    "# CRS = Computer Reservation System\n",
    "# scheduled time as opposed to the actual time\n",
    "print header\n",
    "print filtered_data_2007.take(1)\n",
    "print filtered_data_2008.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "holidays = ['01/01/2007', '01/15/2007', '02/19/2007', '05/28/2007', '06/07/2007', '07/04/2007',\n",
    "      '09/03/2007', '10/08/2007' ,'11/11/2007', '11/22/2007', '12/25/2007',\n",
    "      '01/01/2008', '01/21/2008', '02/18/2008', '05/22/2008', '05/26/2008', '07/04/2008',\n",
    "      '09/01/2008', '10/13/2008' ,'11/11/2008', '11/27/2008', '12/25/2008']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "\n",
    "def make_weather_dict():\n",
    "    weather_dict = {}\n",
    "    weather_lines = weather_data.collect()\n",
    "    for line in weather_lines:\n",
    "        all_vals = line.split(',')\n",
    "        # key: date / values: precipitation, snow, max temp, min temp, average wind\n",
    "        weather_dict[all_vals[2]] = [all_vals[3], all_vals[4], all_vals[5], all_vals[6], all_vals[7]]\n",
    "    \n",
    "    return weather_dict\n",
    "    \n",
    "def days_from_nearest_holiday(year, month, day):\n",
    "    diffs = []\n",
    "    sample_date = datetime.date(year, month, day)\n",
    "    for holiday in holidays:\n",
    "        dt = datetime.datetime.strptime(holiday, '%m/%d/%Y').date()\n",
    "        td = dt - sample_date\n",
    "        diffs.append(abs(td.days))\n",
    "\n",
    "    return min(diffs) * 1.0\n",
    "    \n",
    "def make_labeled_point(flight):\n",
    "    try:\n",
    "        wd = weather_dict.value\n",
    "        flight_data = flight.split(',')\n",
    "        date_string = str(flight_data[0]) + str(flight_data[1]).zfill(2) + str(flight_data[2]).zfill(2)\n",
    "        weather_data = wd[date_string]\n",
    "        features = []\n",
    "        lbl = 0.0\n",
    "        if float(flight_data[15]) >= 15.0:\n",
    "            lbl = 1.0\n",
    "        features.append(float(flight_data[1]))         # Month\n",
    "        features.append(float(flight_data[2]))         # DayofMonth\n",
    "        features.append(float(flight_data[3]))         # DayOfWeek\n",
    "        features.append(float(flight_data[5]) / 100)   # CRSDepTime\n",
    "        #features.append(float(flight_data[18]))        # Distance\n",
    "        features.append(days_from_nearest_holiday(int(flight_data[0]), int(flight_data[1]), int(flight_data[2]))) \n",
    "        features.append(float(weather_data[0]))        # precipitation\n",
    "        features.append(float(weather_data[1]))        # snow (tenths of mm)\n",
    "        features.append(float(weather_data[1]) / 10.0) # max temp (tenths of degrees C)\n",
    "        features.append(float(weather_data[2]) / 10.0) # min temp (tenths of degrees C)\n",
    "        features.append(float(weather_data[3]))        # average wind\n",
    "\n",
    "        return (LabeledPoint(lbl, np.array(features)))\n",
    "    except:\n",
    "        return (LabeledPoint(0.0, [0.0 for i in range(10)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LabeledPoint(0.0, [0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0])]\n",
      "[LabeledPoint(0.0, [0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0])]\n"
     ]
    }
   ],
   "source": [
    "weather_dict = sc.broadcast(make_weather_dict())\n",
    "\n",
    "train_data = filtered_data_2007.map(lambda line: make_labeled_point(line))\n",
    "test_data = filtered_data_2008.map(lambda line: make_labeled_point(line))\n",
    "\n",
    "print train_data.take(1)\n",
    "print test_data.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Modeling with Spark and ML-Lib</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LabeledPoint(0.0, [0.18629212918,0.965213923662,0.28365735349,0.510842876881,0.478387802111,0.15383705441,0.15383705441,0.15383705441,0.0110347254218,-2.89693997398])]\n",
      "[LabeledPoint(1.0, [-0.479840062435,-0.335867476192,-0.459272550115,-0.171327377629,-0.356434988513,2.95493449507,-0.500407574756,-0.500407574756,-0.226859660894,0.0754827702159])]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def rescale(feat):\n",
    "    m = feat.mean()\n",
    "    s = np.std(feat)\n",
    "    r = []\n",
    "    for x in feat:\n",
    "        r.append((x - m) / s)\n",
    "\n",
    "    return np.array(r)\n",
    "\n",
    "scaled_train_data = train_data.map(lambda lp: LabeledPoint(lp.label, rescale(lp.features)))\n",
    "scaled_train_data.cache()\n",
    "\n",
    "scaled_test_data = test_data.map(lambda lp: LabeledPoint(lp.label, rescale(lp.features)))\n",
    "scaled_test_data.cache()\n",
    "\n",
    "print scaled_train_data.take(1)\n",
    "print scaled_test_data.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def eval_metrics(lbl_pred):\n",
    "    tp = float(lbl_pred.filter(lambda lp: lp[0]==1.0 and lp[1]==1.0).count())\n",
    "    tn = float(lbl_pred.filter(lambda lp: lp[0]==0.0 and lp[1]==0.0).count())\n",
    "    fp = float(lbl_pred.filter(lambda lp: lp[0]==1.0 and lp[1]==0.0).count())\n",
    "    fn = float(lbl_pred.filter(lambda lp: lp[0]==0.0 and lp[1]==1.0).count())\n",
    "    \n",
    "    precision = tp / (tp + fp)\n",
    "    recall = tp / (tp + fn)\n",
    "    F_measure = 2 * precision * recall / (precision + recall)\n",
    "    accuracy = (tp + tn) / (tp + tn + fp + fn)\n",
    "    return([tp, tn, fp, fn], [precision, recall, F_measure, accuracy])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Delayed : 179590\n",
      "On time : 509546\n",
      "Percent delays : 26.06 pct\n"
     ]
    }
   ],
   "source": [
    "late = test_data.filter(lambda lp: lp.label == 1.0).count() * 1.0\n",
    "on_time = test_data.filter(lambda lp: lp.label == 0.0).count() * 1.0\n",
    "tot = test_data.count()\n",
    "\n",
    "print('Delayed : %i' % late)\n",
    "print('On time : %i' % on_time)\n",
    "print('Percent delays : %.2f pct' % round((late / tot) * 100, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision : 0.42\n",
      "Recall : 0.37\n",
      "F1 : 0.39\n",
      "Accuracy : 0.70\n"
     ]
    }
   ],
   "source": [
    "from pyspark.mllib.classification import LogisticRegressionWithSGD\n",
    "\n",
    "model_lr = LogisticRegressionWithSGD.train(scaled_train_data, iterations=100)\n",
    "labels_and_predictions = scaled_test_data.map(lambda lp: (model_lr.predict(lp.features), lp.label))\n",
    "metrics = eval_metrics(labels_and_predictions)\n",
    "\n",
    "print('Precision : %.2f' % round(metrics[1][0], 2))\n",
    "print('Recall : %.2f' % round(metrics[1][1], 2))\n",
    "print('F1 : %.2f' % round(metrics[1][2], 2))\n",
    "print('Accuracy : %.2f' % round(metrics[1][3], 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision : 0.40\n",
      "Recall : 0.35\n",
      "F1 : 0.37\n",
      "Accuracy : 0.70\n"
     ]
    }
   ],
   "source": [
    "from pyspark.mllib.classification import SVMWithSGD\n",
    "\n",
    "model_svm = SVMWithSGD.train(scaled_train_data, iterations=100, step=1.0, regParam=0.01)\n",
    "labels_and_predictions = scaled_test_data.map(lambda lp: (model_svm.predict(lp.features), lp.label))\n",
    "metrics = eval_metrics(labels_and_predictions)\n",
    "\n",
    "print('Precision : %.2f' % round(metrics[1][0], 2))\n",
    "print('Recall : %.2f' % round(metrics[1][1], 2))\n",
    "print('F1 : %.2f' % round(metrics[1][2], 2))\n",
    "print('Accuracy : %.2f' % round(metrics[1][3], 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
