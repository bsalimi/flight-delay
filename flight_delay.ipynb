{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Predicting airline delays with Spark and ML-Lib using pySpark </h1>\n",
    "<br>\n",
    "Adapted from http://nbviewer.ipython.org/github/ofermend/IPython-notebooks/blob/master/blog-part-2.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Pre-processing with PySpark</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'Year,Month,DayofMonth,DayOfWeek,DepTime,CRSDepTime,ArrTime,CRSArrTime,UniqueCarrier,FlightNum,TailNum,ActualElapsedTime,CRSElapsedTime,AirTime,ArrDelay,DepDelay,Origin,Dest,Distance,TaxiIn,TaxiOut,Cancelled,CancellationCode,Diverted,CarrierDelay,WeatherDelay,NASDelay,SecurityDelay,LateAircraftDelay']\n",
      "[u'2007,1,1,1,1446,1445,1631,1700,WN,732,N483,225,255,209,-29,1,STL,LAX,1593,7,9,0,,0,0,0,0,0,0']\n",
      "[u'2008,1,3,4,1738,1715,1838,1820,WN,82,N499WN,60,65,42,18,23,LAS,LAX,236,6,12,0,,0,0,0,0,12,6']\n"
     ]
    }
   ],
   "source": [
    "import os.path\n",
    "\n",
    "AIRPORT = 'LAX'\n",
    "\n",
    "base_dir = os.path.join('data')\n",
    "input_path_2007 = os.path.join('flights', '2007.csv')\n",
    "input_path_2008 = os.path.join('flights', '2008.csv')\n",
    "input_path_LAX = os.path.join('flights', 'LAX.csv')\n",
    "\n",
    "file_name_2007 = os.path.join(base_dir, input_path_2007)\n",
    "file_name_2008 = os.path.join(base_dir, input_path_2008)\n",
    "file_name_LAX = os.path.join(base_dir, input_path_LAX)\n",
    "\n",
    "raw_data_2007 = sc.textFile(file_name_2007)\n",
    "raw_data_2008 = sc.textFile(file_name_2008)\n",
    "weather_data_LAX = sc.textFile(file_name_LAX)\n",
    "\n",
    "header = raw_data_2007.take(1) \n",
    "\n",
    "# filter on Airport\n",
    "filtered_data_2007 = (raw_data_2007\n",
    "                        .filter(lambda line: ',' + AIRPORT + ',' in line)\n",
    "                        # filter out cancelled flights\n",
    "                        .filter(lambda line: ',,' in line)\n",
    "                        .filter(lambda line: 'Year' not in line))\n",
    "filtered_data_2008 = (raw_data_2008\n",
    "                        .filter(lambda line: ',' + AIRPORT + ',' in line)\n",
    "                        # filter out cancelled flights\n",
    "                        .filter(lambda line: ',,' in line)\n",
    "                        .filter(lambda line: 'Year' not in line))\n",
    "\n",
    "# CRS = Computer Reservation System\n",
    "# scheduled time as opposed to the actual time\n",
    "print header\n",
    "print filtered_data_2007.take(1)\n",
    "print filtered_data_2008.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "holidays = ['01/01/2007', '01/15/2007', '02/19/2007', '05/28/2007', '06/07/2007', '07/04/2007',\n",
    "      '09/03/2007', '10/08/2007' ,'11/11/2007', '11/22/2007', '12/25/2007',\n",
    "      '01/01/2008', '01/21/2008', '02/18/2008', '05/22/2008', '05/26/2008', '07/04/2008',\n",
    "      '09/01/2008', '10/13/2008' ,'11/11/2008', '11/27/2008', '12/25/2008']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "def make_weather_dict():\n",
    "    weather_dict = {}\n",
    "    weather_lines = weather_data_LAX.collect()\n",
    "    for line in weather_lines:\n",
    "        all_vals = line.split(',')\n",
    "        # key: date, values: wind speed, max temp, min temp, precipitation\n",
    "        weather_dict[all_vals[2]] = [all_vals[3], all_vals[5], all_vals[6], all_vals[7]]\n",
    "    \n",
    "    return weather_dict\n",
    "    \n",
    "def days_from_nearest_holiday(year, month, day):\n",
    "    diffs = []\n",
    "    sample_date = datetime.date(year, month, day)\n",
    "    for holiday in holidays:\n",
    "        dt = datetime.datetime.strptime(holiday, '%m/%d/%Y').date()\n",
    "        td = dt - sample_date\n",
    "        diffs.append(abs(td.days))\n",
    "\n",
    "    return min(diffs) * 1.0\n",
    "\n",
    "def make_features(flight, weather_dict):\n",
    "    try:\n",
    "        flight_data = flight.split(',')\n",
    "        date_string = str(flight_data[0]) + str(flight_data[1]).zfill(2) + str(flight_data[2]).zfill(2)\n",
    "        weather_data = weather_dict[date_string]\n",
    "        features = []\n",
    "        features.append(float(flight_data[15]))\n",
    "        features.append(float(flight_data[1]))\n",
    "        features.append(float(flight_data[2]))\n",
    "        features.append(float(flight_data[3]))\n",
    "        features.append(float(flight_data[5]) / 100)\n",
    "        features.append(float(flight_data[18]))\n",
    "        features.append(days_from_nearest_holiday(int(flight_data[0]), int(flight_data[1]), int(flight_data[2]))) \n",
    "        features.append(float(weather_data[0]))\n",
    "        features.append(float(weather_data[1]) / 10.0)\n",
    "        features.append(float(weather_data[2]) / 10.0)\n",
    "        features.append(float(weather_data[3]))\n",
    "\n",
    "        return (date_string, features)\n",
    "    except:\n",
    "        return (date_string, [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('20070101', [1.0, 1.0, 1.0, 1.0, 14.45, 1593.0, 0.0, 0.0, 17.2, 7.2, 22.0])]\n",
      "[('20080103', [23.0, 1.0, 3.0, 4.0, 17.15, 236.0, 2.0, 0.0, 18.3, 12.2, 24.0])]\n"
     ]
    }
   ],
   "source": [
    "weather_dict = make_weather_dict()\n",
    "\n",
    "features_2007 = filtered_data_2007.map(lambda line: make_features(line, weather_dict))\n",
    "features_2008 = filtered_data_2008.map(lambda line: make_features(line, weather_dict))\n",
    "\n",
    "print features_2007.take(1)\n",
    "print features_2008.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Modeling with Spark and ML-Lib</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LabeledPoint(0.0, [-0.402295571189,0.0,-1.77624109236,-1.43718831265,0.160527403522,1.6084603387,-1.39015824099,-0.323937227318,-0.125063604608,-0.0661563396444,-0.277943099073])]\n",
      "[LabeledPoint(1.0, [0.266621329907,0.171080153528,-1.55046145948,0.100742991539,0.782614848046,-0.690575560979,-1.07685279843,-0.547252069547,0.529045541619,1.5462275009,-0.456680001685])]\n"
     ]
    }
   ],
   "source": [
    "from pyspark.mllib.regression import LabeledPoint\n",
    "from pyspark.mllib.feature import StandardScaler\n",
    "import numpy as np\n",
    "\n",
    "def parse_data(tup):\n",
    "    if tup[1][0] >= 15:\n",
    "        return LabeledPoint(1.0, tup[1])\n",
    "    else:\n",
    "        return LabeledPoint(0.0, tup[1])\n",
    "    \n",
    "def scale(dataset):\n",
    "    standardizer = StandardScaler(True, True)\n",
    "    labels = dataset.map(lambda lp: lp.label).collect()\n",
    "    features = dataset.map(lambda lp: lp.features).collect()\n",
    "    rdd_features = sc.parallelize(features)\n",
    "    model = standardizer.fit(rdd_features)\n",
    "    scaled_features = model.transform(rdd_features).collect()\n",
    "    zipped = zip(labels, scaled_features)\n",
    "    labeled_points = []\n",
    "    for z in zipped:\n",
    "        labeled_points.append(LabeledPoint(z[0], z[1]))\n",
    "    return sc.parallelize(labeled_points)\n",
    "\n",
    "train_data = features_2007.map(parse_data)\n",
    "train_data.cache()\n",
    "scaled_train_data = scale(train_data)\n",
    "scaled_train_data.cache()\n",
    "\n",
    "test_data = features_2008.map(parse_data)\n",
    "test_data.cache()\n",
    "scaled_test_data = scale(test_data)\n",
    "scaled_test_data.cache()\n",
    "\n",
    "print scaled_train_data.take(1)\n",
    "print scaled_test_data.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def eval_metrics(lbl_pred):\n",
    "    tp = float(lbl_pred.filter(lambda lp: lp[0]==1.0 and lp[1]==1.0).count())\n",
    "    tn = float(lbl_pred.filter(lambda lp: lp[0]==0.0 and lp[1]==0.0).count())\n",
    "    fp = float(lbl_pred.filter(lambda lp: lp[0]==1.0 and lp[1]==0.0).count())\n",
    "    fn = float(lbl_pred.filter(lambda lp: lp[0]==0.0 and lp[1]==1.0).count())\n",
    "    \n",
    "    precision = tp / (tp + fp)\n",
    "    recall = tp / (tp + fn)\n",
    "    F_measure = 2 * precision * recall / (precision + recall)\n",
    "    accuracy = (tp + tn) / (tp + tn + fp + fn)\n",
    "    return([tp, tn, fp, fn], [precision, recall, F_measure, accuracy])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision : 0.95\n",
      "Recall : 0.98\n",
      "F1 : 0.97\n",
      "Accuracy : 0.98\n"
     ]
    }
   ],
   "source": [
    "from pyspark.mllib.classification import LogisticRegressionWithSGD\n",
    "\n",
    "model_lr = LogisticRegressionWithSGD.train(scaled_train_data, iterations=100)\n",
    "labels_and_predictions = scaled_test_data.map(lambda lp: (model_lr.predict(lp.features), lp.label))\n",
    "metrics = eval_metrics(labels_and_predictions)\n",
    "\n",
    "print('Precision : %.2f' % round(metrics[1][0], 2))\n",
    "print('Recall : %.2f' % round(metrics[1][1], 2))\n",
    "print('F1 : %.2f' % round(metrics[1][2], 2))\n",
    "print('Accuracy : %.2f' % round(metrics[1][3], 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision : 0.98\n",
      "Recall : 0.99\n",
      "F1 : 0.99\n",
      "Accuracy : 0.99\n"
     ]
    }
   ],
   "source": [
    "from pyspark.mllib.classification import SVMWithSGD\n",
    "\n",
    "model_svm = SVMWithSGD.train(scaled_train_data, iterations=100, step=1.0, regParam=0.01)\n",
    "labels_and_predictions = scaled_test_data.map(lambda lp: (model_svm.predict(lp.features), lp.label))\n",
    "metrics = eval_metrics(labels_and_predictions)\n",
    "\n",
    "print('Precision : %.2f' % round(metrics[1][0], 2))\n",
    "print('Recall : %.2f' % round(metrics[1][1], 2))\n",
    "print('F1 : %.2f' % round(metrics[1][2], 2))\n",
    "print('Accuracy : %.2f' % round(metrics[1][3], 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision : 1.00\n",
      "Recall : 0.70\n",
      "F1 : 0.82\n",
      "Accuracy : 0.92\n"
     ]
    }
   ],
   "source": [
    "from pyspark.mllib.tree import DecisionTree\n",
    "\n",
    "# Experimental\n",
    "model_dt = DecisionTree.trainClassifier(scaled_train_data, 2, {}, 'gini', 10, 100)\n",
    "# this detour is needed because of the spark context error \n",
    "# if we map the test data set like in the previous models\n",
    "list_data = scaled_test_data.collect()\n",
    "list_predictions = []\n",
    "for lp in list_data:\n",
    "    list_predictions.append((model_dt.predict(lp.features), lp.label))\n",
    "\n",
    "labels_and_predictions = sc.parallelize(list_predictions)\n",
    "metrics = eval_metrics(labels_and_predictions)\n",
    "\n",
    "print('Precision : %.2f' % round(metrics[1][0], 2))\n",
    "print('Recall : %.2f' % round(metrics[1][1], 2))\n",
    "print('F1 : %.2f' % round(metrics[1][2], 2))\n",
    "print('Accuracy : %.2f' % round(metrics[1][3], 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
