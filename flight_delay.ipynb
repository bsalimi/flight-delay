{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Predicting airline delays with Spark and ML-Lib using pySpark </h1>\n",
    "<br>\n",
    "Adapted from http://nbviewer.ipython.org/github/ofermend/IPython-notebooks/blob/master/blog-part-2.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Pre-processing with PySpark</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'Year,Month,DayofMonth,DayOfWeek,DepTime,CRSDepTime,ArrTime,CRSArrTime,UniqueCarrier,FlightNum,TailNum,ActualElapsedTime,CRSElapsedTime,AirTime,ArrDelay,DepDelay,Origin,Dest,Distance,TaxiIn,TaxiOut,Cancelled,CancellationCode,Diverted,CarrierDelay,WeatherDelay,NASDelay,SecurityDelay,LateAircraftDelay']\n",
      "[u'2007,1,1,1,1446,1445,1631,1700,WN,732,N483,225,255,209,-29,1,STL,LAX,1593,7,9,0,,0,0,0,0,0,0']\n",
      "[u'2008,1,3,4,1738,1715,1838,1820,WN,82,N499WN,60,65,42,18,23,LAS,LAX,236,6,12,0,,0,0,0,0,12,6']\n"
     ]
    }
   ],
   "source": [
    "import os.path\n",
    "\n",
    "AIRPORT = 'LAX'\n",
    "\n",
    "base_dir = os.path.join('data')\n",
    "input_path_2007 = os.path.join('flights', '2007.csv')\n",
    "input_path_2008 = os.path.join('flights', '2008.csv')\n",
    "input_path_LAX = os.path.join('flights', 'LAX.csv')\n",
    "\n",
    "file_name_2007 = os.path.join(base_dir, input_path_2007)\n",
    "file_name_2008 = os.path.join(base_dir, input_path_2008)\n",
    "file_name_LAX = os.path.join(base_dir, input_path_LAX)\n",
    "\n",
    "raw_data_2007 = sc.textFile(file_name_2007)\n",
    "raw_data_2008 = sc.textFile(file_name_2008)\n",
    "weather_data_LAX = sc.textFile(file_name_LAX)\n",
    "\n",
    "header = raw_data_2007.take(1) \n",
    "\n",
    "filtered_data_2007 = (raw_data_2007\n",
    "                        # filter on Airport\n",
    "                        .filter(lambda line: ',' + AIRPORT + ',' in line)\n",
    "                        # filter out cancelled flights\n",
    "                        .filter(lambda line: ',,' in line)\n",
    "                        .filter(lambda line: 'Year' not in line))\n",
    "filtered_data_2008 = (raw_data_2008\n",
    "                        # filter on Airport\n",
    "                        .filter(lambda line: ',' + AIRPORT + ',' in line)\n",
    "                        # filter out cancelled flights\n",
    "                        .filter(lambda line: ',,' in line)\n",
    "                        .filter(lambda line: 'Year' not in line))\n",
    "\n",
    "# CRS = Computer Reservation System\n",
    "# scheduled time as opposed to the actual time\n",
    "print header\n",
    "print filtered_data_2007.take(1)\n",
    "print filtered_data_2008.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "holidays = ['01/01/2007', '01/15/2007', '02/19/2007', '05/28/2007', '06/07/2007', '07/04/2007',\n",
    "      '09/03/2007', '10/08/2007' ,'11/11/2007', '11/22/2007', '12/25/2007',\n",
    "      '01/01/2008', '01/21/2008', '02/18/2008', '05/22/2008', '05/26/2008', '07/04/2008',\n",
    "      '09/01/2008', '10/13/2008' ,'11/11/2008', '11/27/2008', '12/25/2008']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "def make_weather_dict():\n",
    "    weather_dict = {}\n",
    "    weather_lines = weather_data_LAX.collect()\n",
    "    for line in weather_lines:\n",
    "        all_vals = line.split(',')\n",
    "        # key: date / values: wind speed, max temp, min temp, precipitation\n",
    "        weather_dict[all_vals[2]] = [all_vals[3], all_vals[5], all_vals[6], all_vals[7]]\n",
    "    \n",
    "    return weather_dict\n",
    "    \n",
    "def days_from_nearest_holiday(year, month, day):\n",
    "    diffs = []\n",
    "    sample_date = datetime.date(year, month, day)\n",
    "    for holiday in holidays:\n",
    "        dt = datetime.datetime.strptime(holiday, '%m/%d/%Y').date()\n",
    "        td = dt - sample_date\n",
    "        diffs.append(abs(td.days))\n",
    "\n",
    "    return min(diffs) * 1.0\n",
    "\n",
    "def make_features(flight, weather_dict):\n",
    "    try:\n",
    "        flight_data = flight.split(',')\n",
    "        date_string = str(flight_data[0]) + str(flight_data[1]).zfill(2) + str(flight_data[2]).zfill(2)\n",
    "        weather_data = weather_dict[date_string]\n",
    "        features = []\n",
    "        features.append(float(flight_data[15]))        # DepDelay\n",
    "        features.append(float(flight_data[1]))         # Month\n",
    "        features.append(float(flight_data[2]))         # DayofMonth\n",
    "        features.append(float(flight_data[3]))         # DayOfWeek\n",
    "        features.append(float(flight_data[5]) / 100)   # CRSDepTime\n",
    "        #features.append(float(flight_data[18]))        # Distance\n",
    "        features.append(days_from_nearest_holiday(int(flight_data[0]), int(flight_data[1]), int(flight_data[2]))) \n",
    "        features.append(float(weather_data[0]))        # wind speed\n",
    "        features.append(float(weather_data[1]) / 10.0) # max temp\n",
    "        features.append(float(weather_data[2]) / 10.0) # min temp\n",
    "        features.append(float(weather_data[3]))        # precipitation\n",
    "\n",
    "        return (date_string, np.array(features))\n",
    "    except:\n",
    "        return (date_string, [i for i in range(11)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('20070101', [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10])]\n",
      "[('20080103', [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10])]\n"
     ]
    }
   ],
   "source": [
    "weather_dict = make_weather_dict()\n",
    "\n",
    "features_2007 = filtered_data_2007.map(lambda line: make_features(line, weather_dict))\n",
    "features_2008 = filtered_data_2008.map(lambda line: make_features(line, weather_dict))\n",
    "\n",
    "print features_2007.take(1)\n",
    "print features_2008.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Modeling with Spark and ML-Lib</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LabeledPoint(0.0, [-0.694398239158,-0.694398239158,-0.694398239158,-0.694398239158,1.00836499087,-0.820997735814,-0.820997735814,1.35651360667,0.0905186401091,1.96419119062])]\n",
      "[LabeledPoint(0.0, [-0.694398239158,-0.694398239158,-0.694398239158,-0.694398239158,1.00836499087,-0.820997735814,-0.820997735814,1.35651360667,0.0905186401091,1.96419119062])]\n"
     ]
    }
   ],
   "source": [
    "from pyspark.mllib.regression import LabeledPoint\n",
    "from pyspark.mllib.feature import StandardScaler\n",
    "import numpy as np\n",
    "\n",
    "def rescale(feat):\n",
    "    m = feat.mean()\n",
    "    s = np.std(feat)\n",
    "    r = []\n",
    "    for x in feat:\n",
    "        r.append((x - m) / s)\n",
    "\n",
    "    return np.array(r)\n",
    "\n",
    "train_data = features_2007.map(lambda t: LabeledPoint(1.0, t[1]) if t[1][0] >= 15 else LabeledPoint(0.0, t[1]))\n",
    "train_data.cache()\n",
    "scaled_train_data = train_data.map(lambda lp: LabeledPoint(lp.label, rescale(lp.features)))\n",
    "scaled_train_data.cache()\n",
    "\n",
    "test_data = features_2008.map(lambda t: LabeledPoint(1.0, t[1]) if t[1][0] >= 15 else LabeledPoint(0.0, t[1]))\n",
    "test_data.cache()\n",
    "scaled_test_data = train_data.map(lambda lp: LabeledPoint(lp.label, rescale(lp.features)))\n",
    "scaled_test_data.cache()\n",
    "\n",
    "print scaled_train_data.take(1)\n",
    "print scaled_test_data.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def eval_metrics(lbl_pred):\n",
    "    tp = float(lbl_pred.filter(lambda lp: lp[0]==1.0 and lp[1]==1.0).count())\n",
    "    tn = float(lbl_pred.filter(lambda lp: lp[0]==0.0 and lp[1]==0.0).count())\n",
    "    fp = float(lbl_pred.filter(lambda lp: lp[0]==1.0 and lp[1]==0.0).count())\n",
    "    fn = float(lbl_pred.filter(lambda lp: lp[0]==0.0 and lp[1]==1.0).count())\n",
    "    \n",
    "    precision = tp / (tp + fp)\n",
    "    recall = tp / (tp + fn)\n",
    "    F_measure = 2 * precision * recall / (precision + recall)\n",
    "    accuracy = (tp + tn) / (tp + tn + fp + fn)\n",
    "    return([tp, tn, fp, fn], [precision, recall, F_measure, accuracy])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Delayed : 36178\n",
      "On time : 148417\n",
      "Percent delays : 19.60 pct\n"
     ]
    }
   ],
   "source": [
    "late = test_data.filter(lambda lp: lp.label == 1.0).count() * 1.0\n",
    "on_time = test_data.filter(lambda lp: lp.label == 0.0).count() * 1.0\n",
    "tot = test_data.count()\n",
    "\n",
    "print('Delayed : %i' % late)\n",
    "print('On time : %i' % on_time)\n",
    "print('Percent delays : %.2f pct' % round((late / tot) * 100, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision : 0.99\n",
      "Recall : 0.84\n",
      "F1 : 0.91\n",
      "Accuracy : 0.97\n"
     ]
    }
   ],
   "source": [
    "from pyspark.mllib.classification import LogisticRegressionWithSGD\n",
    "\n",
    "model_lr = LogisticRegressionWithSGD.train(scaled_train_data, iterations=100)\n",
    "labels_and_predictions = scaled_test_data.map(lambda lp: (model_lr.predict(lp.features), lp.label))\n",
    "metrics = eval_metrics(labels_and_predictions)\n",
    "\n",
    "print('Precision : %.2f' % round(metrics[1][0], 2))\n",
    "print('Recall : %.2f' % round(metrics[1][1], 2))\n",
    "print('F1 : %.2f' % round(metrics[1][2], 2))\n",
    "print('Accuracy : %.2f' % round(metrics[1][3], 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision : 0.98\n",
      "Recall : 0.92\n",
      "F1 : 0.95\n",
      "Accuracy : 0.98\n"
     ]
    }
   ],
   "source": [
    "from pyspark.mllib.classification import SVMWithSGD\n",
    "\n",
    "model_svm = SVMWithSGD.train(scaled_train_data, iterations=100, step=1.0, regParam=0.01)\n",
    "labels_and_predictions = scaled_test_data.map(lambda lp: (model_svm.predict(lp.features), lp.label))\n",
    "metrics = eval_metrics(labels_and_predictions)\n",
    "\n",
    "print('Precision : %.2f' % round(metrics[1][0], 2))\n",
    "print('Recall : %.2f' % round(metrics[1][1], 2))\n",
    "print('F1 : %.2f' % round(metrics[1][2], 2))\n",
    "print('Accuracy : %.2f' % round(metrics[1][3], 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
